{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://page.kakao.com'\n",
    "options = Options()\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "data_list = []\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "driver.execute_script(f\"document.querySelector('button.pr-16pxr').click();\")\n",
    "time.sleep(3)\n",
    "\n",
    "id_input = driver.find_element(By.CSS_SELECTOR, '#loginId--1')\n",
    "id_input.send_keys('tkdehdjajsl@gmail.com')\n",
    "pw_input = driver.find_element(By.CSS_SELECTOR, '#password--2')\n",
    "pw_input.send_keys('gksruf9507')\n",
    "time.sleep(5)\n",
    "\n",
    "submit_btn = driver.find_element(By.CSS_SELECTOR, 'button.submit')\n",
    "submit_btn.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# 로그인 후 쿠키 저장\n",
    "cookies = driver.get_cookies()\n",
    "with open('kakao_cookies.pkl', 'wb') as f:\n",
    "  pickle.dump(cookies, f)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://page.kakao.com'\n",
    "options = Options()\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "data_list = []\n",
    "\n",
    "# 크롬 드라이버 초기화\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "\n",
    "# 쿠키 로드\n",
    "with open('kakao_cookies.pkl', 'rb') as f:\n",
    "    cookies = pickle.load(f)\n",
    "    for cookie in cookies:\n",
    "        driver.add_cookie(cookie)\n",
    "\n",
    "driver.refresh()\n",
    "time.sleep(3)\n",
    "\n",
    "# Data.csv와 Links.csv 파일 로드\n",
    "data_df = pd.read_csv('Data.csv')\n",
    "links_df = pd.read_csv('Links.csv')\n",
    "\n",
    "# 링크를 순회하며 데이터 추출\n",
    "for link in links_df['link']:\n",
    "  data = {}\n",
    "\n",
    "  try:\n",
    "    driver.get(link)\n",
    "    time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('span.font-large3-bold.mb-3pxr.text-ellipsis.break-all.text-el-70.line-clamp-2').text\n",
    "    author = soup.select_one('span.font-small2.mb-6pxr.text-ellipsis.text-el-70.opacity-70.break-word-anywhere.line-clamp-2').text\n",
    "    genre = soup.select('span.break-all.align-middle')[1].text\n",
    "\n",
    "    # Data.csv와 비교하여 일치하는 경우에만 데이터 추가\n",
    "    match = data_df[(data_df['title'] == title) & (data_df['author'] == author) & (data_df['genre'] == genre)]\n",
    "    if not match.empty:\n",
    "      data['title'] = title\n",
    "      data['author'] = author\n",
    "      data['genre'] = genre\n",
    "      data['src'] = soup.select_one('div.jsx-1044487760.image-container.relative > img')['src']\n",
    "      data_list.append(data)\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing {link}: {e}\")\n",
    "    continue\n",
    "\n",
    "# 데이터프레임으로 변환 및 저장\n",
    "df_result = pd.DataFrame(data_list)\n",
    "output_path = 'imgs.csv'\n",
    "df_result.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://page.kakao.com'\n",
    "options = Options()\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "data_list = []\n",
    "\n",
    "# 여기에 로그인 파트\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# 쿠키 로드\n",
    "with open('kakao_cookies.pkl', 'rb') as f:\n",
    "  cookies = pickle.load(f)\n",
    "  for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "\n",
    "driver.refresh()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "df = pd.read_csv('Links.csv')\n",
    "for url in df['link'].iloc[20001:40001]:\n",
    "  data = {}\n",
    "\n",
    "  try:\n",
    "    # 두번째 페이지 먼저 호출\n",
    "    driver.get(url+'?tab_type=about')\n",
    "    time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "      \n",
    "    # JavaScript를 사용하여 버튼이 존재하면 클릭\n",
    "    script = f\"\"\"\n",
    "    var button = document.querySelector('button.font-medium1.flex.h-59pxr.cursor-pointer.items-center.justify-center.border-0.border-t-1.border-solid.border-line-10.px-24pxr.text-el-50');\n",
    "    if (button) {{\n",
    "        button.click();\n",
    "    }}\n",
    "    \"\"\"\n",
    "    driver.execute_script(script)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 키워드 정보가 없거나, 발행자 정보가 한개 이상이거나, 또는 전자책 정가 정보가 없으면 패스\n",
    "    keywords = soup.select('div.flex.w-full.flex-wrap.px-18pxr.pb-10pxr > a > div > span')\n",
    "    if not keywords:\n",
    "      continue\n",
    "\n",
    "    others = soup.select('div.flex.w-full.flex-col.items-start.px-18pxr.pb-18pxr > div')[2:]\n",
    "    if len(others) != 3:\n",
    "      continue\n",
    "\n",
    "    check = others[0].select('span.break-all.align-middle')\n",
    "    if check:\n",
    "      continue\n",
    "\n",
    "    title = soup.select_one('span.font-large3-bold.mb-3pxr.text-ellipsis.break-all.text-el-70.line-clamp-2').text\n",
    "    if '단행본' in title:\n",
    "      continue\n",
    "    else:\n",
    "      data['title'] = title\n",
    "      \n",
    "    # 첫번째 페이지 호출\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "      \n",
    "    for keyword in keywords:\n",
    "      data['keywords'] = ''.join(keyword.text for keyword in keywords)\n",
    "\n",
    "    data['publisher'] = others[0].select('span')[1].text\n",
    "    data['users'] = others[1].select('span')[1].text\n",
    "    data['price'] = others[2].select('span')[1].text\n",
    "\n",
    "    author = soup.select_one('span.font-small2.mb-6pxr.text-ellipsis.text-el-70.opacity-70.break-word-anywhere.line-clamp-2').text\n",
    "    data['author'] = author\n",
    "    ing = soup.select_one('div.mt-6pxr.flex.items-center > span.font-small2.text-el-70.opacity-70').text\n",
    "    data['ing'] = ing\n",
    "    genre = soup.select('span.break-all.align-middle')[1].text\n",
    "    data['genre'] = genre\n",
    "    view = soup.select('span.text-el-70.opacity-70')[1].text\n",
    "    data['viewCount'] = view\n",
    "    score = soup.select('span.text-el-70.opacity-70')[2].text\n",
    "    data['score'] = score\n",
    "    counts = soup.select('span.text-ellipsis.break-all.line-clamp-1.font-small2-bold.text-el-70')\n",
    "    uploaded = counts[0].text\n",
    "    data['uploaded'] = uploaded\n",
    "    review = counts[1].text\n",
    "    data['reviewCount'] = review\n",
    "    date = soup.select_one('div.flex.flex-col.pr-14pxr > div > span.break-all.align-middle').text\n",
    "    data['date'] = date\n",
    "\n",
    "    free_info = soup.select_one('div.flex.flex-wrap.text-el-70 > span.opacity-70.font-small2').text\n",
    "    data['free_info'] = free_info\n",
    "    \n",
    "    data_list.append(data)\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "df1 = pd.DataFrame(data_list)\n",
    "df1.to_csv('Data120001-40001.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://page.kakao.com/menu/10011/screen/84'\n",
    "options = Options()\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "data_list = []\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "data = []\n",
    "try:\n",
    "  driver.get(url)\n",
    "  time.sleep(3)\n",
    "\n",
    "  last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "  for _ in range(1):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "  html = driver.page_source\n",
    "  soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "  atags = soup.select('div.flex.w-full.grow.flex-col.px-122pxr > div > div.flex.grow.flex-col > div.mb-4pxr.flex-col > div > div.flex.grow.flex-col > div > div > div > div > div > a')\n",
    "  \n",
    "  for atag in atags:\n",
    "    img = atag.select_one('img')['src']\n",
    "    title = atag.select_one('div.jsx-3256825605.font-small1.break-all.text-el-60.line-clamp-2').text\n",
    "    data.append({'img': img, 'title': title})\n",
    "    \n",
    "except:\n",
    "  print('Error occurred')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('imgs.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 불러오기\n",
    "data_existing = pd.read_csv('Data.csv')\n",
    "\n",
    "# 새로 가져온 데이터 불러오기\n",
    "data_new = pd.read_csv('imgs.csv')\n",
    "\n",
    "# title 열을 기준으로 데이터 비교하여 일치하는 데이터 필터링\n",
    "matched_data = data_new[data_new['title'].isin(data_existing['title'])]\n",
    "\n",
    "# 필터링된 데이터 저장\n",
    "matched_data.to_csv('matched_imgs.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단행본 처리\n",
    "df = df[~df['title'].apply(lambda x: bool(re.search(r'단행본', x)))]\n",
    "# 연재와 단행본이 중복되기에 단행본은 제거합니다.\n",
    "\n",
    "# viewCount 전처리\n",
    "def convert_view_count(value):\n",
    "  if ',' in value:\n",
    "    value = value.replace(',', '')\n",
    "  if '만' in value:\n",
    "    value = value.replace('만', '')\n",
    "    \n",
    "    return int(float(value) * 10000)\n",
    "  if '억' in value:\n",
    "    value = value.replace('억', '')\n",
    "    \n",
    "    return int(float(value) * 10000000)\n",
    "  else:\n",
    "    return int(value)\n",
    "# 조회수는 최소 0부터 00억뷰 까지 존재합니다.\n",
    "# ',' 처리 후 '만' 혹은 '억'이 존재하면 제거 후 형변환하여 리턴합니다.\n",
    "  \n",
    "df['viewCount'] = df['viewCount'].apply(convert_view_count)\n",
    "\n",
    "# reviewCount 전처리\n",
    "def convert_review_count(value):\n",
    "  value = value.replace('전체 ', '')\n",
    "  if ',' in value:\n",
    "    value = value.replace(',', '')\n",
    "\n",
    "  if '만' in value:\n",
    "    value = value.replace('만', '')\n",
    "    return int(float(value) * 10000)\n",
    "  else:\n",
    "    return int(value)\n",
    "# 리뷰 수는 0부터 00만 단위까지 존재합니다. viewCount와 같이 전처리를 수행합니다.\n",
    "  \n",
    "df['reviewCount'] = df['reviewCount'].apply(convert_review_count)\n",
    "\n",
    "# 연재/완결 전처리\n",
    "def convert_ing(value):\n",
    "  if '연재' in value:\n",
    "    value = value.replace(value, '연재')\n",
    "    return value\n",
    "  else:\n",
    "    return value\n",
    "  \n",
    "# 연재 00화 혹은 완결 => 연재/완결 전처리합니다.\n",
    "\n",
    "df['ing'] = df['ing'].apply(convert_ing)\n",
    "\n",
    "# 업로드 화 전처리\n",
    "def convert_uploaded(value):\n",
    "  if ',' in value:\n",
    "    value = value.replace(',', '')\n",
    "\n",
    "  if '전체' in value:\n",
    "    value = value.replace('전체 ', '')\n",
    "    \n",
    "  return int(value)\n",
    "\n",
    "# str형태의 전체 00를 숫자만 남도록 전처리\n",
    "\n",
    "df['uploaded'] = df['uploaded'].apply(convert_uploaded)\n",
    "\n",
    "# Price 컬럼 전처리\n",
    "def convert_price(value):\n",
    "  if '원' in value:\n",
    "    match = re.match(r'(\\d+)', value)\n",
    "\n",
    "    if match:\n",
    "      return int(match.group(1))\n",
    "    else:\n",
    "      return 0\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "# 100원/200원 숫자만 남도록 전처리\n",
    "\n",
    "df['price'] = df['price'].apply(convert_price)\n",
    "df = df[(df.price <= 200) & (df.price > 0)]\n",
    "\n",
    "# date 전처리\n",
    "df['date'] = pd.to_datetime(df['date'], format='%y.%m.%d')\n",
    "\n",
    "# reaction컬럼 = (u_cnt+reviewCount)/2\n",
    "df['reaction'] = (df['viewCount'] + df['reviewCount']) / 2\n",
    "# 시리즈/카카오페이지 각 플랫폼 작품들의 작품 인기도를 측정하기 위하여\n",
    "# 각 데이터의 유의미한 요소를 합하여 '독자 반응 컬럼' 생성\n",
    "\n",
    "# 장르별로 그룹화하여 reviewCount가 높은 순으로 rank 컬럼 생성\n",
    "def rank_by_reviewCount(group):\n",
    "  # value.reviewCount가 가장 높으면 rank 1\n",
    "  # value.reviewCount가 낮아질수록 rank + 1\n",
    "  group = group.sort_values(by=['reaction', 'title'], ascending=[False, True]).reset_index(drop=True)\n",
    "  group['rank'] = group.index + 1\n",
    "  return group\n",
    "\n",
    "ranked_df = df.groupby('genre').apply(rank_by_reviewCount).reset_index(drop=True)\n",
    "\n",
    "# free_info 전처리\n",
    "def convert_free_info(value):\n",
    "  match = re.match(r'(\\d+)', value)\n",
    "\n",
    "  if match:\n",
    "    return int(match.group(1))\n",
    "  \n",
    "df['free_info'] = df['free_info'].apply(convert_free_info)\n",
    "\n",
    "# 키워드 전처리\n",
    "def extract_keywords(row):\n",
    "  keywords = row.split('#')[1:]\n",
    "  remove_keywords = ['로맨스판타지', '현대로맨스']\n",
    "  \n",
    "  keywords = [keyword for keyword in keywords if keyword not in remove_keywords]\n",
    "  return keywords\n",
    "\n",
    "#df['keywords'] = df['keywords'].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['score'] = data['score'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_score(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the 'score' column\n",
    "data['score'] = data['score'].apply(clean_score)\n",
    "\n",
    "# Drop rows with NaN values in the 'score' column\n",
    "data = data.dropna(subset=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CREATE TABLE users (\n",
    "    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "    users VARCHAR(100) NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE publishers (\n",
    "    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "    publisher VARCHAR(255) NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE genre (\n",
    "    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "    genre VARCHAR(255) NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE keywords (\n",
    "    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "    keywords JSON NOT NULL,\n",
    "    genre_id INT UNSIGNED,\n",
    "    FOREIGN KEY (genre_id) REFERENCES genre(id)\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE novels (\n",
    "    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "    title VARCHAR(255) NOT NULL,\n",
    "    users_id INT UNSIGNED,\n",
    "    publisher_id INT UNSIGNED,\n",
    "    price SMALLINT UNSIGNED,\n",
    "    ing VARCHAR(50),\n",
    "    author VARCHAR(100),\n",
    "    genre_id INT UNSIGNED,\n",
    "    viewCount BIGINT UNSIGNED,\n",
    "    reviewCount BIGINT UNSIGNED,\n",
    "    score FLOAT,\n",
    "    uploaded INT UNSIGNED,\n",
    "    date DATE,\n",
    "    free_info TINYINT UNSIGNED,\n",
    "    keywords JSON,\n",
    "    FOREIGN KEY (users_id) REFERENCES users(id),\n",
    "    FOREIGN KEY (publisher_id) REFERENCES publisher(id),\n",
    "    FOREIGN KEY (genre_id) REFERENCES genre(id)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publisers 테이블에 넣을 값 생성 코드\n",
    "\n",
    "# 'publisher' 열의 고유 값 추출\n",
    "unique_publishers = data['publisher'].unique()\n",
    "\n",
    "# INSERT 문 생성\n",
    "insert_statements = []\n",
    "for publisher in unique_publishers:\n",
    "    insert_statements.append(f\"INSERT INTO publishers (publisher) VALUES ('{publisher}');\")\n",
    "\n",
    "# 결과 출력\n",
    "for statement in insert_statements:\n",
    "    print(statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords 테이블에 넣을 데이터 생성 코드\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'Data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract unique keywords\n",
    "keywords_set = set()\n",
    "for keywords in data['keywords']:\n",
    "    if pd.notna(keywords):\n",
    "        keywords_list = keywords.split('#')\n",
    "        keywords_set.update([kw.strip() for kw in keywords_list if kw.strip()])\n",
    "\n",
    "unique_keywords = list(keywords_set)\n",
    "\n",
    "sql_insert_keywords = \"INSERT INTO keywords (keyword) VALUES \"\n",
    "values = \", \".join([f\"('{kw}')\" for kw in unique_keywords])\n",
    "sql_insert_keywords += values + \";\"\n",
    "\n",
    "print(sql_insert_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import json\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "# Novels 테이블 값 INSERT 코드\n",
    "\n",
    "data = pd.read_csv('Data.csv')\n",
    "\n",
    "# MySQL 데이터베이스에 연결\n",
    "try:\n",
    "  conn = mysql.connector.connect(\n",
    "    host='localhost',       # MySQL 호스트 (포트 번호 제외)\n",
    "    user='root',   # MySQL 사용자 이름\n",
    "    password='mysql',  # MySQL 비밀번호\n",
    "    database='team1'\n",
    "  )\n",
    "  cursor = conn.cursor()\n",
    "except mysql.connector.Error as err:\n",
    "  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "    print(\"사용자 이름 또는 비밀번호가 잘못되었습니다.\")\n",
    "  elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "    print(\"데이터베이스가 존재하지 않습니다.\")\n",
    "  else:\n",
    "    print(err)\n",
    "\n",
    "# ID를 가져오거나 없으면 삽입하고 ID 반환하는 함수\n",
    "def get_or_insert_id(table, column, value):\n",
    "  cursor.execute(f\"SELECT id FROM {table} WHERE {column} = %s\", (value,))\n",
    "  result = cursor.fetchone()\n",
    "  if result:\n",
    "      return result[0]\n",
    "  cursor.execute(f\"INSERT INTO {table} ({column}) VALUES (%s)\", (value,))\n",
    "  conn.commit()\n",
    "  return cursor.lastrowid\n",
    "\n",
    "# keywords를 유효한 JSON 형식으로 변환하는 함수\n",
    "def keywords_to_json(value):\n",
    "  keywords_list = [k.strip() for k in value.split('#') if k.strip()]\n",
    "  return json.dumps(keywords_list)\n",
    "\n",
    "# novels 테이블에 데이터 삽입\n",
    "for index, row in data.iterrows():\n",
    "  users_id = get_or_insert_id('users', 'users', row['users'])\n",
    "  publisher_id = get_or_insert_id('publishers', 'publisher', row['publisher'])\n",
    "  genre_id = get_or_insert_id('genre', 'genre', row['genre'])\n",
    "\n",
    "  # keywords 컬럼 값을 유효한 JSON 형식으로 변환\n",
    "  keywords = keywords_to_json(row['keywords'])\n",
    "\n",
    "  cursor.execute(\"\"\"\n",
    "    INSERT INTO novels (title, users_id, publisher_id, price, ing, author, genre_id, viewCount, reviewCount, score, uploaded, date, free_info, keywords)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\", (\n",
    "    row['title'], users_id, publisher_id, row['price'], row['ing'], row['author'], genre_id,\n",
    "    row['viewCount'], row['reviewCount'], row['score'], row['uploaded'], row['date'],\n",
    "    row['free_info'], keywords\n",
    "  ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# 데이터베이스 연결 설정\n",
    "db_uri = 'mysql+pymysql://root:mysql@localhost/team1'  # 이 부분을 실제 사용자 이름과 비밀번호로 수정해야 합니다.\n",
    "engine = create_engine(db_uri)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# 기존 데이터 불러오기\n",
    "data = pd.read_csv('Data.csv')\n",
    "\n",
    "# 장르별로 키워드를 #으로 분할\n",
    "grouped = data.groupby('genre')['keywords'].apply(lambda x: '#'.join(x)).reset_index()\n",
    "grouped['keywords'] = grouped['keywords'].apply(lambda x: [kw for kw in set(x.split('#')) if kw])\n",
    "\n",
    "# 장르 테이블 데이터 수동 정의\n",
    "genre_dict = {\n",
    "    '판타지': 1,\n",
    "    '로판': 2,\n",
    "    '현판': 3,\n",
    "    '로맨스': 4,\n",
    "    'BL': 5,\n",
    "    '무협': 6,\n",
    "    '드라마': 7\n",
    "}\n",
    "\n",
    "# 메타데이터 리플렉션\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "keywords_table = metadata.tables['keywords']\n",
    "\n",
    "insert_data = []\n",
    "\n",
    "# 키워드 데이터 준비 및 삽입\n",
    "for _, row in grouped.iterrows():\n",
    "  genre = row['genre']\n",
    "  if genre in genre_dict:\n",
    "    genre_id = genre_dict[genre]\n",
    "    keywords = row['keywords']\n",
    "    for keyword in keywords:\n",
    "      insert_data.append({'keyword': keyword, 'genre_id': genre_id})\n",
    "\n",
    "# 키워드 데이터베이스에 삽입 및 커밋\n",
    "with engine.begin() as connection:  # `begin` 사용하여 자동으로 커밋 또는 롤백 처리\n",
    "  connection.execute(keywords_table.insert(), insert_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
