{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링(웹 크롤링)\n",
    "크롤링이란\n",
    "- 인터넷 상의 웹 페이지 데이터를 자동으로 수집하는 과정.\n",
    "- 웹 크롤링은 일반적으로 웹 스크래핑과 연관되며, 둘은 종종 혼용되지만 조금 다른 개념. 웹 크롤링은 웹 페이지를 탐색하고 데이터를 수집하는 반면, 웹 스크래핑은 그 페이지에서 특정 정보를 추출하는 데 중점을 둔다.\n",
    "- 크롤링은 스크래핑을 포함할 수 있다. 크롤링 과정에서 각 페이지를 방문할 때, 스크래핑을 통해 필요한 데이터를 추출할 수 있다.\n",
    "\n",
    "웹 크롤링에 사용되는 도구\n",
    "- BeautifulSoup: Python 라이브러리로, HTML 및 XML 문서를 구문 분석하고 데이터를 추출하는 데 사용.\n",
    "\n",
    "- Scrapy: 웹 크롤링을 위한 Python 프레임워크로, 효율적이고 확장성이 높은 크롤러를 쉽게 만들 수 있다.\n",
    "\n",
    "- Selenium: 웹 브라우저 자동화 도구로, JavaScript가 동적으로 로드되는 페이지를 크롤링할 때 유용.\n",
    "\n",
    "- Requests: Python의 HTTP 라이브러리로, 웹 페이지 요청을 쉽게 할 수 있다.\n",
    "\n",
    "웹 크롤링의 기본 과정\n",
    "- 크롤러 설정: 크롤러는 특정 웹 페이지를 시작점으로 설정. 이를 '시드(seed)'라고 부르며, 크롤러는 이 시드 URL에서 시작해 다른 페이지로 이동.\n",
    "\n",
    "- 페이지 요청: 크롤러는 HTTP 요청을 보내 웹 페이지를 요청. 이 과정에서 크롤러는 브라우저처럼 행동하여 웹 서버에서 페이지를 가져온다.\n",
    "\n",
    "- 데이터 추출: 웹 페이지가 응답되면, 크롤러는 페이지의 HTML을 분석하고 필요한 데이터를 추출. 이 과정에는 BeautifulSoup, Selenium 같은 라이브러리가 사용될 수 있다.\n",
    "\n",
    "- 링크 추출 및 큐잉: 크롤러는 현재 페이지에서 다른 페이지로 연결되는 링크를 추출하고, 이 링크들을 큐(queue)에 추가하여 다음 크롤링 대상으로 삼는다.\n",
    "\n",
    "- 반복: 이 과정은 정해진 규칙이나 종료 조건이 충족될 때까지 반복. 예를 들어, 특정 수의 페이지를 크롤링하거나, 주어진 도메인 내에서만 크롤링하도록 설정할 수 있다.\n",
    "\n",
    "웹 크롤링의 주의사항\n",
    "\n",
    "- 로봇 배제 표준(robots.txt): 많은 웹사이트는 robots.txt 파일을 통해 크롤러가 접근 가능한 부분과 접근을 제한하는 부분을 명시. 크롤러는 이 규칙을 준수해야 한다.\n",
    "\n",
    "- 저작권 및 법적 이슈: 모든 웹사이트의 콘텐츠는 저작권의 보호를 받는다. 따라서 크롤링을 통해 수집한 데이터를 어떻게 사용할지에 대한 법적 문제를 주의해야 한다.\n",
    "\n",
    "- 서버 부하: 지나친 크롤링은 웹 서버에 부하를 줄 수 있다. 크롤링 시에는 서버의 부담을 줄이기 위해 요청 간의 딜레이를 설정하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup\n",
    "- BeautifulSoup은 HTML이나 XML 문서를 파싱하고, 파싱한 데이터에서 원하는 요소를 검색하고 추출하는 데 매우 유용한 도구입니다. \n",
    "- BeautifulSoup에서 객체를 찾는 주요 방법에는 find, find_all, select_one, select, find_parents, find_parent, find_next_sibling, find_previous_sibling 등이 있습니다.\n",
    "\n",
    "검색 방식\n",
    "- find, find_all: 태그 이름과 속성을 사용하여 요소를 검색합니다. (a, p, attr 등)\n",
    "- select_one, select: CSS 선택자를 사용하여 요소를 검색합니다. (id, class 등)\n",
    "\n",
    "반환 결과:\n",
    "- find: 첫 번째로 일치하는 요소를 반환합니다.\n",
    "- find_all: 모든 일치하는 요소를 리스트로 반환합니다.\n",
    "- select_one: 첫 번째로 일치하는 요소를 반환합니다.\n",
    "- select: 모든 일치하는 요소를 리스트로 반환합니다.\n",
    "\n",
    "표현력:\n",
    "- select_one, select: 더 복잡하고 정교한 선택 조건을 지정할 수 있습니다. 예를 들어, CSS 선택자 문법을 사용하여 클래스, ID, 속성 등을 조합한 검색이 가능합니다.\n",
    "- find, find_all: 단순한 태그 이름과 속성 조건에 기반한 검색이 주로 사용됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html.parser vs. lxml\n",
    "- 파이썬에서 HTML 및 XML 문서를 파싱(parsing)하는 라이브러리\n",
    "- html.parser는 HTML 문서를 파싱하는 데에 적합한 파서. 파이썬의 기본 라이브러리로 제공되며 파이썬 내부적으로 구현되어 있으며, 외부 종속성이 없으므로 파이썬과 함께 설치되는 패키지만 사용할 수 있습니다.\n",
    "- lxml은 C 언어로 작성된 파이썬 외부 라이브러리로서 HTML 및 XML 문서를 파싱하는 데에 적합하며, 파서 성능이 매우 우수합니다.\n",
    "- HTML 문서를 파싱하는 경우에는 html.parser를 사용하는 것이 간단하고 편리하며, 대부분의 경우에는 충분한 성능을 제공합니다. 그러나 대용량의 XML 문서나 매우 복잡한 HTML 문서를 파싱해야 하는 경우에는 lxml을 사용하는 것이 더 효율적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = '<html><body><h1>Title</h1><p class=\"content\">First paragraph.</p><p class=\"content\">Second paragraph.</p></body></html>'\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링 시 헤더를 포함하면 성공적으로 데이터를 가져올 가능성을 높여준다.\n",
    "\n",
    "headers : HTTP 요청에 포함되는 메타데이터\n",
    "- User-Agent: 클라이언트 애플리케이션(브라우저 등)을 나타냅니다.\n",
    "- Accept: 서버가 어떤 콘텐츠 타입을 반환해야 하는지 지정합니다.\n",
    "- Accept-Language: 클라이언트가 선호하는 언어를 지정합니다.\n",
    "- Referer: 요청이 발생한 이전 페이지의 URL을 지정합니다.\n",
    "- Host: 요청을 보내는 서버의 호스트 이름을 지정합니다.\n",
    "- Connection: 서버와 클라이언트 간의 연결 유형을 지정합니다.\n",
    "\n",
    "```\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': 'http://example.com',\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 예시 네이버 뉴스 페이지 URL\n",
    "url = 'https://news.naver.com'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "res = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response.content:\n",
    "- response.content는 서버에서 반환된 응답을 바이트(byte) 문자열로 제공합니다.\n",
    "- 주로 이미지, 파일 다운로드와 같은 바이너리 데이터를 다룰 때 사용됩니다.\n",
    "- 인코딩과 상관없이 원본 그대로의 데이터를 가져오기 때문에, HTML 파싱을 할 때는 별도로 인코딩을 지정하지 않으면 기본 인코딩을 사용합니다.\n",
    "\n",
    "response.text:\n",
    "- response.text는 서버에서 반환된 응답을 유니코드 문자열로 제공합니다.\n",
    "- requests 라이브러리는 response.text를 반환할 때, response.encoding에 지정된 인코딩을 사용하여 바이트 데이터를 유니코드 문자열로 디코딩합니다.\n",
    "- 일반적인 텍스트 데이터, HTML, JSON 등의 처리를 할 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.content\n",
    "\n",
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'네이버 뉴스'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(res.text, 'html.parser') # content은 디코딩을 해서 출력해줌\n",
    "#print(soup.prettify())\n",
    "\n",
    "# title 태그 요소 찾기\n",
    "soup.find('title').text\n",
    "#soup.find('title').get_text()\n",
    "#soup.find('title').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 언론사별\n",
      "2: 정치\n",
      "3: 경제\n",
      "4: 사회\n",
      "5: 생활/문화\n",
      "6: IT/과학\n",
      "7: 세계\n",
      "8: 랭킹\n",
      "9: 신문보기\n",
      "10: 오피니언\n",
      "11: TV\n",
      "12: 팩트체크\n",
      "13: 알고리즘 안내\n",
      "14: 정정보도 모음\n"
     ]
    }
   ],
   "source": [
    "cats = soup.find_all(class_='Nitem_link_menu')\n",
    "for idx, cat in enumerate(cats):\n",
    "    print(f\"{idx+1}: {cat.get_text().strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
