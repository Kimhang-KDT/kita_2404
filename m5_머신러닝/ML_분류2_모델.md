## DecisionTreeClassifier
  - Decision Tree : 의사결정나무
  - Pruning(가지치기) : 맨 마지막 leaf 노드들이 root 노드까지 가는데 걸리는 조건(edge)의 개수인 깊이(depth)를 제한
  - 각 분기점마다 목표 값을 잘 분류할 수 있는 최선의 속성을 찾아서 배치한다.
  - 하이퍼파라메터 : 매개변수를 설정하여 모델 구축 방식에 영향을 주어 구조와 성능을 향상시킨다.
    - max_depth: 트리 최대 깊이를 설정하여 과적합 방지
    - min_samples_split : 분할 고려를 위해 노드에 있어야하는 최소 샘플 수를 정의. 값이 낮을 수록 데이터에 노이즈가 포착 될 수 있음 주의
    - min_samples_leaf : 분할 후 리프 노드에 있어야하는 최소 샘플 수 설정. 과적합 제어를 위한 파라메터로 값이 높을수록 샘플이 거의 없는 리프노드는 생성되지 않는다.
    - max_features : 노드 분할을 위해 고려되는 최대 기능 수. 이 수를 줄이면 다양성은 증가될 수 있으나 중요 기능이 제외될 가능성이 있다.
    - max_leaf_nodes : 트리의 최대 리프 노드 수. 리프 노드가 많을수록 더 복잡한 모델이 가능.
  - 결정 트리를 사용하여 데이터 레이블의 중요도를 산정. 유의미한 레이블만 산출하여 Registic 등 다른 모델에서 학습 시키는 방법이 있다.

## 앙상블 학습
  - 보팅, 배깅, 부스팅 세가지로 나눌 수 있으며 이외에도 스태깅 등 여러 앙상븡 방법이 있다.
  - 배깅과 부스팅을 주로 사용한다.
  - Random Forest : 배깅의 대표 방식
  - LightGBM : 부스팅의 가장 최신 방식