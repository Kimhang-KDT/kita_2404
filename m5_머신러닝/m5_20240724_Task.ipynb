{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task1_0724. Wine 데이터셋에 대하여 SVM 모델에 3개의 커널을 적용하여 학습 및 평가 결과를 출력하세요."
      ],
      "metadata": {
        "id": "FYQsTcjPRS24"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1TlGk5dRQdu",
        "outputId": "d85ad47d-bba9-4674-c11f-94099d0b41cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.97\n",
            "Accuracy: 0.97\n",
            "Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "for kernel in kernels:\n",
        "  svm = SVC(kernel=kernel, random_state=42)\n",
        "  svm.fit(X_train, y_train)\n",
        "  y_pred = svm.predict(X_test)\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task2_0724. breast_cancer dataset으로 랜덤포레스트를 적용하여 모델링 및 평가를 아래의 하이퍼 파라미터를 이용하여 수행한 후 최적의 하이퍼파라미터를 구하세요.\n",
        "\n",
        "- 'n_estimators': [50, 100, 200],\n",
        "- 'max_depth': [None, 10, 20],\n",
        "- 'max_features': ['auto', 'sqrt', 'log2'],\n",
        "- 'min_samples_split': [2, 5, 10],\n",
        "- 'min_samples_leaf': [1, 2, 4]"
      ],
      "metadata": {
        "id": "t8nEAISyk3j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#clf = RandomForestClassifier(n_estimators=100, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(n_estimators=200, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(n_estimators=50, random_state=42) # 9649\n",
        "\n",
        "#clf = RandomForestClassifier(max_depth=None, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(max_depth=10, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(max_depth=20, random_state=42) # 9649\n",
        "\n",
        "#clf = RandomForestClassifier(max_features='auto', random_state=42) # warning\n",
        "#clf = RandomForestClassifier(max_features='sqrt', random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(max_features='log2', random_state=42) # 9649\n",
        "\n",
        "#clf = RandomForestClassifier(min_samples_split=2, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(min_samples_split=5, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(min_samples_split=10, random_state=42) # 9649\n",
        "\n",
        "#clf = RandomForestClassifier(min_samples_leaf=1, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(min_samples_leaf=2, random_state=42) # 9649\n",
        "#clf = RandomForestClassifier(min_samples_leaf=4, random_state=42) # 9649\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=50, max_depth=20, max_features='sqrt', min_samples_split=5, min_samples_leaf=2, random_state=42)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "print(f\"Accuracy : {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "PBqAl0CWk3xb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de770927-d5ff-46ce-f26b-8e13e9d49520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, Binarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 1. 데이터 로드\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "           'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "# na_values = ? 는 ?로 되어있는 값들을 None값으로 처리한다는 의미\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values='?', skipinitialspace=True)\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "Q1 = data['fnlwgt'].quantile(0.25)\n",
        "Q3 = data['fnlwgt'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_fnlwgt_outliers = data[(data['fnlwgt'] < lower_bound) | (data['fnlwgt'] > upper_bound)]\n",
        "data = data.drop(capital_fnlwgt_outliers.index)\n",
        "\n",
        "Q1 = data['capital-gain'].quantile(0.25)\n",
        "Q3 = data['capital-gain'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_gain_outliers = data[(data['capital-gain'] < lower_bound) | (data['capital-gain'] > upper_bound)]\n",
        "capital_loss_outliers = data[(data['capital-loss'] < lower_bound) | (data['capital-loss'] > upper_bound)]\n",
        "data = data.drop(capital_gain_outliers.index)\n",
        "data = data.drop(capital_loss_outliers.index)\n",
        "\n",
        "# 범주형 변수 인코딩\n",
        "categorical_features = ['race', 'sex', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'native-country', 'income']\n",
        "data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "# 변수 선택 및 데이터 분리\n",
        "\n",
        "X = data.drop('income_>50K', axis=1)\n",
        "y = data['income_>50K']\n",
        "\n",
        "# 파생변수1 : age_group\n",
        "data['age_group'] = pd.cut(data['age'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '61+'])\n",
        "\n",
        "# 파생변수2 : hours_group\n",
        "data['hours_group'] = pd.cut(data['hours-per-week'], bins=[0, 20, 40, 60, 100], labels=['0-20', '21-40', '41-60', '61+'])\n",
        "\n",
        "# 파생변수3 : capital\n",
        "data['capital'] = data['capital-gain'] - data['capital-loss']\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA_8-q5TsNd-",
        "outputId": "cd0937f3-abb6-4c0c-d783-10a4a732e57f",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best parameters for XGBoost: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "[LightGBM] [Info] Number of positive: 4070, number of negative: 16245\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016665 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 643\n",
            "[LightGBM] [Info] Number of data points in the train set: 20315, number of used features: 77\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.200345 -> initscore=-1.384142\n",
            "[LightGBM] [Info] Start training from score -1.384142\n",
            "Best parameters for LightGBM: {'learning_rate': 0.2, 'n_estimators': 50, 'num_leaves': 31}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "print(f\"Best parameters for Logistic Regression: {grid_search_lr.best_params_}\")\n",
        "\n",
        "\n",
        "########################################\n",
        "param_grid_dt = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "print(f\"Best parameters for Decision Tree: {grid_search_dt.best_params_}\")\n",
        "\n",
        "\n",
        "######################################\n",
        "param_grid_gbm = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "grid_search_gbm = GridSearchCV(GradientBoostingClassifier(), param_grid_gbm, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_gbm.fit(X_train, y_train)\n",
        "print(f\"Best parameters for GBM: {grid_search_gbm.best_params_}\")\n",
        "\n",
        "######################################\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "print(f\"Best parameters for SVM: {grid_search_svm.best_params_}\")\n",
        "\n",
        "\n",
        "################################\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "print(f\"Best parameters for KNN: {grid_search_knn.best_params_}\")"
      ],
      "metadata": {
        "id": "6S_tlplzI-ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "grid_search_xgb = GridSearchCV(xgb.XGBClassifier(), param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "print(f\"Best parameters for XGBoost: {grid_search_xgb.best_params_}\")\n",
        "\n",
        "######################################\n",
        "param_grid_lgb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'num_leaves': [31, 50, 100]\n",
        "}\n",
        "\n",
        "grid_search_lgb = GridSearchCV(lgb.LGBMClassifier(), param_grid_lgb, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_lgb.fit(X_train, y_train)\n",
        "print(f\"Best parameters for LightGBM: {grid_search_lgb.best_params_}\")"
      ],
      "metadata": {
        "id": "Lo-MJY5VCOW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, Binarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# 1. 데이터 로드\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "           'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "# na_values = ? 는 ?로 되어있는 값들을 None값으로 처리한다는 의미\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values='?', skipinitialspace=True)\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "Q1 = data['fnlwgt'].quantile(0.25)\n",
        "Q3 = data['fnlwgt'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_fnlwgt_outliers = data[(data['fnlwgt'] < lower_bound) | (data['fnlwgt'] > upper_bound)]\n",
        "data = data.drop(capital_fnlwgt_outliers.index)\n",
        "\n",
        "Q1 = data['capital-gain'].quantile(0.25)\n",
        "Q3 = data['capital-gain'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_gain_outliers = data[(data['capital-gain'] < lower_bound) | (data['capital-gain'] > upper_bound)]\n",
        "capital_loss_outliers = data[(data['capital-loss'] < lower_bound) | (data['capital-loss'] > upper_bound)]\n",
        "data = data.drop(capital_gain_outliers.index)\n",
        "data = data.drop(capital_loss_outliers.index)\n",
        "\n",
        "# 범주형 변수 인코딩\n",
        "categorical_features = ['race', 'sex', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'native-country', 'income']\n",
        "data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "# 변수 선택 및 데이터 분리\n",
        "\n",
        "X = data.drop('income_>50K', axis=1)\n",
        "y = data['income_>50K']\n",
        "\n",
        "# 파생변수1 : age_group\n",
        "data['age_group'] = pd.cut(data['age'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '61+'])\n",
        "\n",
        "# 파생변수2 : hours_group\n",
        "data['hours_group'] = pd.cut(data['hours-per-week'], bins=[0, 20, 40, 60, 100], labels=['0-20', '21-40', '41-60', '61+'])\n",
        "\n",
        "# 파생변수3 : capital\n",
        "data['capital'] = data['capital-gain'] - data['capital-loss']\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# BestParameter\n",
        "model = RandomForestClassifier(max_depth= 20, max_features= 'sqrt', min_samples_leaf= 1, min_samples_split= 10, n_estimators= 200, random_state=10)\n",
        "#{'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred = model.predict(X_test)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 정확도 평가\n",
        "print(f\"Matrix: {conf_matrix}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Report: {class_report}\")\n",
        "print(f\"Accuracy: {accuracy_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD_XheUHAP1z",
        "outputId": "ceac8dd9-c73d-4473-e950-251e049a19fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix: [[3934  183]\n",
            " [ 538  424]]\n",
            "Accuracy: 0.8580\n",
            "Report:               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.88      0.96      0.92      4117\n",
            "        True       0.70      0.44      0.54       962\n",
            "\n",
            "    accuracy                           0.86      5079\n",
            "   macro avg       0.79      0.70      0.73      5079\n",
            "weighted avg       0.85      0.86      0.84      5079\n",
            "\n",
            "Accuracy: <function accuracy_score at 0x7861e8187be0>\n"
          ]
        }
      ]
    }
  ]
}