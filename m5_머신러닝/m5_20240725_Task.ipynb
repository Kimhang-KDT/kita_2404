{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task1_0725. 타이타닉 생존자 예측 데이터 세트 train.csv에 대하여 다음 사항을 수행하세요.\n",
        "- 일괄 전처리 사용자 함수 transform_features(df) 작성\n",
        "- 분류 모델 학습 및 평가 사용자 함수 작성\n",
        "- dt, lr, rf 모델링 및 평가(정확도)\n",
        "\n",
        "- GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행.\n",
        "  - Decision Tree, Random Forest, Logistic Regression 모델별 수행\n",
        "  - 선택한 모델에 적합한 parameter greed 적용\n",
        "  - cv=5 적용"
      ],
      "metadata": {
        "id": "IE2-xf_9TrDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def categorize_age(age):\n",
        "  if age < 13:\n",
        "      return 'Child'\n",
        "  elif age < 20:\n",
        "      return 'Teenager'\n",
        "  elif age < 60:\n",
        "      return 'Adult'\n",
        "  else:\n",
        "      return 'Senior'\n",
        "\n",
        "# 일괄 전처리 사용자 함수 transform_features(df)\n",
        "def transform_features(df):\n",
        "  # 이상치 처리\n",
        "  Q1 = df['Fare'].quantile(0.25)\n",
        "  Q3 = df['Fare'].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  fare_outliers = df[(df['Fare'] < (Q1 - 1.5 * IQR)) | (df['Fare'] > (Q3 + 1.5 * IQR))]\n",
        "\n",
        "  df = df.drop(fare_outliers.index)\n",
        "\n",
        "  # 결측치 처리\n",
        "  imputer_most_frequent = SimpleImputer(strategy='most_frequent')\n",
        "  df['Age'] = imputer_most_frequent.fit_transform(df[['Age']])\n",
        "  df['Fare'] = imputer_most_frequent.fit_transform(df[['Fare']])\n",
        "  df['Embarked'] = df['Embarked'].fillna('S')\n",
        "\n",
        "  # has_family 컬럼 생성\n",
        "  df['family_size'] = df['SibSp'] + df['Parch']\n",
        "\n",
        "  df['AgeGroup'] = df['Age'].apply(lambda x: categorize_age(x))\n",
        "\n",
        "  df['Pclass_Fare'] = df['Pclass'] * df['Fare']\n",
        "\n",
        "  df['TicketCount'] = df.groupby('Ticket')['Ticket'].transform('count')\n",
        "\n",
        "  # 원-핫 인코딩\n",
        "  df = pd.get_dummies(df, columns=['Embarked', 'Sex', 'SibSp', 'Parch', 'family_size', 'AgeGroup', 'TicketCount', 'Ticket'])\n",
        "\n",
        "  # 필요없는 데이터\n",
        "  df.drop(columns=['PassengerId', 'Name', 'Cabin'], inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/KDT_2404/dataset/train.csv')\n",
        "\n",
        "df = transform_features(df)\n",
        "\n",
        "# 변수 선택 및 데이터 분리\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "df.drop(columns=['Survived'], inplace=True)\n",
        "\n",
        "# 8. 학습용과 테스트용 데이터셋으로 나누기\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
        "\n",
        "# 7. 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# 모델 및 하이퍼파라미터 설정\n",
        "models = {\n",
        "    'Logistic Regression': (LogisticRegression(max_iter=1000), {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
        "    }),\n",
        "    'Decision Tree': (DecisionTreeClassifier(), {\n",
        "        'max_depth': [None, 10, 20, 30, 40],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }),\n",
        "    'Random Forest': (RandomForestClassifier(), {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    })\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 하이퍼파라미터 튜닝 및 모델 학습\n",
        "for model_name, (model, params) in models.items():\n",
        "    grid_search = GridSearchCV(model, params, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
        "    results[model_name] = {\n",
        "        'Best Parameters': grid_search.best_params_,\n",
        "        'Accuracy': accuracy,\n",
        "        'ROC AUC': roc_auc\n",
        "    }\n",
        "\n",
        "# 결과 출력\n",
        "for model_name, result in results.items():\n",
        "    print(f'{model_name} - Best Parameters: {result[\"Best Parameters\"]}, Accuracy: {result[\"Accuracy\"]}, ROC AUC: {result[\"ROC AUC\"]}')\n",
        "\n",
        "# 결과 (5분 소요)\n",
        "# Logistic Regression - Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8774193548387097, ROC AUC: 0.8546666666666667\n",
        "# Decision Tree - Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.864516129032258, ROC AUC: 0.7828571428571429\n",
        "# Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}, Accuracy: 0.8580645161290322, ROC AUC: 0.8727619047619047\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "\n",
        "# 가장 베스트 값으로 하이퍼파라메터 튜닝\n",
        "# models = {\n",
        "#   'Logistic Regression': LogisticRegression(C= 0.1, solver= 'saga'),\n",
        "#   'Decision Tree': DecisionTreeClassifier(max_depth= 10, min_samples_leaf= 4, min_samples_split= 5),\n",
        "#   'Random Forest': RandomForestClassifier(max_depth= 10, min_samples_leaf= 4, min_samples_split= 5)\n",
        "# }\n",
        "\n",
        "# # 10. 모델 학습 및 평가\n",
        "# for name, model in models.items():\n",
        "#   model.fit(X_train, y_train)\n",
        "#   y_pred = model.predict(X_test)\n",
        "#   accuracy = accuracy_score(y_test, y_pred)\n",
        "#   conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#   class_report = classification_report(y_test, y_pred)\n",
        "#   roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "#   print(f'Model: {name}')\n",
        "#   print(f'Accuracy: {accuracy:.4f}')\n",
        "#   print('Confusion Matrix:')\n",
        "#   print(conf_matrix)\n",
        "#   print('Classification Report:')\n",
        "#   print(class_report)\n",
        "#   print(f'ROC AUC: {roc_auc:.4f}')\n",
        "#   print('\\n' + '='*60 + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzijk_Mk-ybG",
        "outputId": "dab553cf-4bba-48b2-9639-4599b9a96cd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8774193548387097, ROC AUC: 0.8546666666666667\n",
            "Decision Tree - Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.864516129032258, ROC AUC: 0.7828571428571429\n",
            "Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}, Accuracy: 0.8580645161290322, ROC AUC: 0.8727619047619047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, Binarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. 데이터 로드\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "           'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "# na_values = ? 는 ?로 되어있는 값들을 None값으로 처리한다는 의미\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values='?', skipinitialspace=True)\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "Q1 = data['fnlwgt'].quantile(0.25)\n",
        "Q3 = data['fnlwgt'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_fnlwgt_outliers = data[(data['fnlwgt'] < lower_bound) | (data['fnlwgt'] > upper_bound)]\n",
        "data = data.drop(capital_fnlwgt_outliers.index)\n",
        "\n",
        "Q1 = data['capital-gain'].quantile(0.25)\n",
        "Q3 = data['capital-gain'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_gain_outliers = data[(data['capital-gain'] < lower_bound) | (data['capital-gain'] > upper_bound)]\n",
        "capital_loss_outliers = data[(data['capital-loss'] < lower_bound) | (data['capital-loss'] > upper_bound)]\n",
        "data = data.drop(capital_gain_outliers.index)\n",
        "data = data.drop(capital_loss_outliers.index)\n",
        "\n",
        "# 범주형 변수 인코딩\n",
        "categorical_features = ['race', 'sex', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'native-country']\n",
        "data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "# 변수 선택 및 데이터 분리\n",
        "# 'income' 변수를 0과 1로 변환\n",
        "data['income'] = data['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)\n",
        "X = data.drop('income', axis=1)\n",
        "y = data['income']\n",
        "\n",
        "# 파생변수1 : age_group\n",
        "data['age_group'] = pd.cut(data['age'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '61+'])\n",
        "\n",
        "# 파생변수2 : hours_group\n",
        "data['hours_group'] = pd.cut(data['hours-per-week'], bins=[0, 20, 40, 60, 100], labels=['0-20', '21-40', '41-60', '61+'])\n",
        "\n",
        "# 파생변수3 : capital\n",
        "data['capital'] = data['capital-gain'] - data['capital-loss']\n",
        "\n",
        "# 불필요 레이블 삭제\n",
        "data.drop(columns=['age', 'fnlwgt', 'education-num', 'income', 'capital-gain', 'capital-loss', 'hours-per-week'], inplace=True)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "  'Logistic Regression': LogisticRegression(C= 0.1, solver= 'saga'),\n",
        "  'Decision Tree': DecisionTreeClassifier(max_depth= 10, min_samples_leaf= 2, min_samples_split= 10),\n",
        "  'Random Forest': RandomForestClassifier(max_depth= 10, min_samples_leaf= 2, min_samples_split= 10)\n",
        "}\n",
        "\n",
        "# 10. 모델 학습 및 평가\n",
        "for name, model in models.items():\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  class_report = classification_report(y_test, y_pred)\n",
        "  roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "  print(f'Model: {name}')\n",
        "  print(f'Accuracy: {accuracy:.4f}')\n",
        "  print('Confusion Matrix:')\n",
        "  print(conf_matrix)\n",
        "  print('Classification Report:')\n",
        "  print(class_report)\n",
        "  print(f'ROC AUC: {roc_auc:.4f}')\n",
        "  print('\\n' + '='*60 + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G2zG08JCoGU",
        "outputId": "fd07c339-e5bc-432d-ae0f-b85522ce1aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Logistic Regression\n",
            "Accuracy: 0.8573\n",
            "Confusion Matrix:\n",
            "[[3889  228]\n",
            " [ 497  465]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.91      4117\n",
            "           1       0.67      0.48      0.56       962\n",
            "\n",
            "    accuracy                           0.86      5079\n",
            "   macro avg       0.78      0.71      0.74      5079\n",
            "weighted avg       0.85      0.86      0.85      5079\n",
            "\n",
            "ROC AUC: 0.7140\n",
            "\n",
            "============================================================\n",
            "\n",
            "Model: Decision Tree\n",
            "Accuracy: 0.8466\n",
            "Confusion Matrix:\n",
            "[[3935  182]\n",
            " [ 597  365]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91      4117\n",
            "           1       0.67      0.38      0.48       962\n",
            "\n",
            "    accuracy                           0.85      5079\n",
            "   macro avg       0.77      0.67      0.70      5079\n",
            "weighted avg       0.83      0.85      0.83      5079\n",
            "\n",
            "ROC AUC: 0.6676\n",
            "\n",
            "============================================================\n",
            "\n",
            "Model: Random Forest\n",
            "Accuracy: 0.8539\n",
            "Confusion Matrix:\n",
            "[[3968  149]\n",
            " [ 593  369]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91      4117\n",
            "           1       0.71      0.38      0.50       962\n",
            "\n",
            "    accuracy                           0.85      5079\n",
            "   macro avg       0.79      0.67      0.71      5079\n",
            "weighted avg       0.84      0.85      0.84      5079\n",
            "\n",
            "ROC AUC: 0.6737\n",
            "\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
        "Best parameters for Logistic Regression: {'C': 0.1, 'solver': 'saga'}\n",
        "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
        "Best parameters for Decision Tree: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
        "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
        "Best parameters for GBM: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
        "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
        "Best parameters for SVM: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
        "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
        "Best parameters for KNN: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n",
        "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
        "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
        "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}"
      ],
      "metadata": {
        "id": "JzzAxTWlbE42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Null 처리 함수\n",
        "def fillna(df):\n",
        "    df['Age'].fillna(df['Age'].mean(),inplace=True)\n",
        "    df['Cabin'].fillna('N',inplace=True)\n",
        "    df['Embarked'].fillna('N',inplace=True)\n",
        "    df['Fare'].fillna(0,inplace=True)\n",
        "    return df\n",
        "\n",
        "# 머신러닝 알고리즘에 불필요한 속성 제거\n",
        "def drop_features(df):\n",
        "    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n",
        "    return df\n",
        "\n",
        "# 레이블 인코딩 수행.\n",
        "def format_features(df):\n",
        "    df['Cabin'] = df['Cabin'].str[:1]\n",
        "    features = ['Cabin','Sex','Embarked']\n",
        "    for feature in features:\n",
        "        le = LabelEncoder()\n",
        "        le = le.fit(df[feature])\n",
        "        df[feature] = le.transform(df[feature])\n",
        "    return df\n",
        "\n",
        "# 앞에서 설정한 Data Preprocessing 함수 호출\n",
        "def transform_features(df):\n",
        "    df = fillna(df)\n",
        "    df = drop_features(df)\n",
        "    df = format_features(df)\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U3BdKhA3BJLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 사용자 정의 함수\n",
        "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f'오차 행렬:\\n{confusion}')\n",
        "    print(f'정확도: {accuracy:.4f}')\n",
        "    print(f'정밀도: {precision:.4f}')\n",
        "    print(f'재현율: {recall:.4f}')\n",
        "    print(f'F1 스코어: {f1:.4f}')\n",
        "    print(f'ROC AUC: {roc_auc:.4f}')\n",
        "    print('')"
      ],
      "metadata": {
        "id": "hKofbqEIBN0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 데이터를 재로딩 하고, feature데이터 셋과 Label 데이터 셋 추출.\n",
        "\n",
        "y_titanic_df = titanic_df['Survived']\n",
        "X_titanic_df= titanic_df.drop('Survived',axis=1)\n",
        "\n",
        "X_titanic_df = transform_features(X_titanic_df)"
      ],
      "metadata": {
        "id": "eGljkCQOBP4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11)\n",
        "X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11, stratify=y_titanic_df)"
      ],
      "metadata": {
        "id": "bBVup3IBBQ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성\n",
        "dt_clf = DecisionTreeClassifier(random_state=10)\n",
        "rf_clf = RandomForestClassifier(random_state=10)\n",
        "lr_clf = LogisticRegression(max_iter=2000, random_state=10)\n",
        "print('dt_clf 학습')\n",
        "print('='*12)\n",
        "train_and_evaluate(dt_clf, X_train, X_test, y_train, y_test)\n",
        "print('rf_clf 학습')\n",
        "print('='*12)\n",
        "train_and_evaluate(rf_clf, X_train, X_test, y_train, y_test)\n",
        "print('lr_clf 학습')\n",
        "print('='*12)\n",
        "train_and_evaluate(lr_clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "M-WFrVYzBSIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'n_estimators':[10,100,200], 'max_depth':[2,3,5,10,12],\n",
        "             'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8,10]}\n",
        "\n",
        "grid_rfclf = GridSearchCV(rf_clf , param_grid=parameters , scoring='accuracy' , cv=5)\n",
        "grid_rfclf.fit(X_train , y_train)\n",
        "\n",
        "\n",
        "print('GridSearchCV 최적 하이퍼 파라미터 :',grid_rfclf.best_params_)\n",
        "print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_rfclf.best_score_))\n",
        "best_rfclf = grid_rfclf.best_estimator_\n",
        "\n",
        "train_and_evaluate(best_rfclf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "xxQKZFvaBTOE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}