{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task1_0726. 신용카드 사기 검출 모델을 아래와 같이 생성하고 평가하세요.\n",
        "\n",
        "- 데이터 일차 가공 및 모델 학습/예측/평가\n",
        "  - Time 컬럼 삭제, 로지스틱 회귀, LightGBM을 이용하여 모델링 및 평가\n",
        "\n",
        "- Amount 컬럼 데이터 분포도 변환 후 모델 학습/예측/평가\n",
        "  - 표준화한 후 로지스틱 회귀, LightGBM을 이용하여 모델링 및 평가\n",
        "\n",
        "- 이상치 데이터 제거 후 모델 학습/예측/평가\n",
        "  - 데이터의 상관관계를 시각화 V14와 클래스의 상관관계 높음을 확인 후 V14 컬럼의 이상치 제거한 후 로지스틱 회귀, LightGBM을 이용하여 모델링 및 평가\n",
        "\n",
        "- SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가\n",
        "  - 불균형한 데이터셋 처리를 위한 imbalanced-learn 라이브러리를 설치\n",
        "  - %pip install imbalanced-learn\n",
        "  - SMOTE(Synthetic Minority Over-sampling Technique)를 사용하여 불균형한 데이터셋을 처리\n",
        "  - from imblearn.over_sampling import SMOTE\n",
        "  - smote = SMOTE(random_state=0)\n",
        "  - X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
        "  - SMOTE를 적용한 학습 데이터셋을 사용하여 로지스틱 회귀 모델을 학습하고 예측 성능을 평가\n",
        "  - Precision-Recall 커브를 시각화하는 함수\n",
        "  - LightGBM을 이용하여 모델링 및 평가"
      ],
      "metadata": {
        "id": "E_cazYxE1Orq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg-h6i721Kcv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/KDT_2404/dataset/creditcard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환\n",
        "def get_preprocessed_df(df=None):\n",
        "    df_copy = df.copy()\n",
        "    df_copy.drop('Time', axis=1, inplace=True)\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "-EN_e_kT5H8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수.\n",
        "# 데이터를 전처리하는 get_preprocessed_df 함수\n",
        "def get_train_test_dataset(df=None):\n",
        "    # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환\n",
        "    df_copy = get_preprocessed_df(df)\n",
        "    # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들\n",
        "    X_features = df_copy.iloc[:, :-1]\n",
        "    y_target = df_copy.iloc[:, -1]\n",
        "    # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)\n",
        "    # 학습과 테스트 데이터 세트 반환\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)"
      ],
      "metadata": {
        "id": "7eskiEjy5Otw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "Qm08IxdL5W47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('학습 데이터 레이블 값 비율')\n",
        "print(y_train.value_counts()/y_train.shape[0] * 100)\n",
        "print('테스트 데이터 레이블 값 비율')\n",
        "print(y_test.value_counts()/y_test.shape[0] * 100)"
      ],
      "metadata": {
        "id": "Ilu6fFV_5phX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
        "    confusion = confusion_matrix( y_test, pred)\n",
        "    accuracy = accuracy_score(y_test , pred)\n",
        "    precision = precision_score(y_test , pred)\n",
        "    recall = recall_score(y_test , pred)\n",
        "    f1 = f1_score(y_test,pred)\n",
        "    # ROC-AUC 추가\n",
        "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
        "    print('오차 행렬')\n",
        "    print(confusion)\n",
        "    # ROC-AUC print 추가\n",
        "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
        "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
      ],
      "metadata": {
        "id": "dhnhe_bL5tRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 로지스틱 회귀 모델 학습 및 평가"
      ],
      "metadata": {
        "id": "SVy4kohU52AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression(max_iter=100, solver='lbfgs', random_state=42)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 3장에서 사용한 get_clf_eval() 함수를 이용하여 평가 수행.\n",
        "get_clf_eval(y_test, lr_pred, lr_pred_proba)"
      ],
      "metadata": {
        "id": "NCnIq1ov56w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인자로 사이킷런의 Estimator객체와, 학습/테스트 데이터 세트를 입력 받아서 학습/예측/평가 수행.\n",
        "def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
        "    model.fit(ftr_train, tgt_train)\n",
        "    pred = model.predict(ftr_test)\n",
        "    pred_proba = model.predict_proba(ftr_test)[:, 1]\n",
        "    get_clf_eval(tgt_test, pred, pred_proba)"
      ],
      "metadata": {
        "id": "0fSD6F4k59BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LightGBM 모델 학습 및 평가"
      ],
      "metadata": {
        "id": "RAUxiXlJ5_i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "2TEBzvvu6ITG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터 분포도 변환 후 모델 학습/예측/평가"
      ],
      "metadata": {
        "id": "V8zGeOC8Dijl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 신용카드 거래 금액('Amount')의 히스토그램을 생성\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.xticks(range(0, 30000, 1000), rotation=60)\n",
        "sns.histplot(card_df['Amount'], bins=100, kde=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HlAOJ4QoDaKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 표준화"
      ],
      "metadata": {
        "id": "oeU4h1bBDryG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# 사이킷런의 StandardScaler를 이용하여 정규분포 형태로 Amount 피처값 변환하는 로직으로 수정.\n",
        "def get_preprocessed_df(df=None):\n",
        "    df_copy = df.copy()\n",
        "    scaler = StandardScaler()\n",
        "    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1))\n",
        "    # 변환된 Amount를 Amount_Scaled로 피처명 변경후 DataFrame맨 앞 컬럼으로 입력\n",
        "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
        "    # 기존 Time, Amount 피처 삭제\n",
        "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "1bcL_SWLDnc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Amount를 정규분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행.\n",
        "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
        "\n",
        "print('### 로지스틱 회귀 예측 성능 ###')\n",
        "lr_clf = LogisticRegression()\n",
        "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
        "\n",
        "print('### LightGBM 예측 성능 ###')\n",
        "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "5emKuRqIDtkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 로그변환"
      ],
      "metadata": {
        "id": "tHT7v_-CFoKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.log1p 함수를 사용하여 'Amount' 피처를 로그 변환\n",
        "def get_preprocessed_df(df=None):\n",
        "    df_copy = df.copy()\n",
        "    # 넘파이의 log1p( )를 이용하여 Amount를 로그 변환\n",
        "    amount_n = np.log1p(df_copy['Amount'])\n",
        "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
        "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "2AWg4NN5Fpn0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 셀에서는 로그 변환된 'Amount' 피처를 사용하여 로지스틱 회귀와 LightGBM 모델을 다시 학습하고 예측 성능을 평가\n",
        "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
        "\n",
        "print('### 로지스틱 회귀 예측 성능 ###')\n",
        "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
        "\n",
        "print('### LightGBM 예측 성능 ###')\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "FyTuE1KTFtUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 이상치 데이터 제거 후 평가"
      ],
      "metadata": {
        "id": "mvajo0ZHEmiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터의 상관 관계를 시각화 : v14, v17이 class와 상관관계가 높음\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(9, 9))\n",
        "corr = card_df.corr()\n",
        "sns.heatmap(corr, cmap='RdBu')"
      ],
      "metadata": {
        "id": "7HobO7XkDyec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터에서 이상치를 찾는 get_outlier 함수를 정의\n",
        "import numpy as np\n",
        "\n",
        "def get_outlier(df=None, column=None, weight=1.5):\n",
        "    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함.\n",
        "    fraud = df[df['Class']==1][column]\n",
        "    quantile_25 = np.percentile(fraud.values, 25)\n",
        "    quantile_75 = np.percentile(fraud.values, 75)\n",
        "    # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함.\n",
        "    iqr = quantile_75 - quantile_25\n",
        "    iqr_weight = iqr * weight\n",
        "    lowest_val = quantile_25 - iqr_weight\n",
        "    highest_val = quantile_75 + iqr_weight\n",
        "    # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환.\n",
        "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
        "    return outlier_index"
      ],
      "metadata": {
        "id": "dJaLVe9yEyWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_outlier 함수를 호출하여 'V14' 피처의 이상치 인덱스를 찾고, 이를 출력\n",
        "outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)\n",
        "print('이상치 데이터 인덱스:', outlier_index)"
      ],
      "metadata": {
        "id": "ljyc6kSeEzWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_processed_df( )를 로그 변환 후 V14 피처의 이상치 데이터를 삭제하는 로직으로 변경.\n",
        "def get_preprocessed_df(df=None):\n",
        "    df_copy = df.copy()\n",
        "    amount_n = np.log1p(df_copy['Amount'])\n",
        "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
        "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "    # 이상치 데이터 삭제하는 로직 추가\n",
        "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
        "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
        "    return df_copy\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
        "print('### 로지스틱 회귀 예측 성능 ###')\n",
        "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
        "print('### LightGBM 예측 성능 ###')\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "j2WN8SdOE5HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가\n"
      ],
      "metadata": {
        "id": "dquUeKUZE98j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 불균형한 데이터셋 처리를 위한 imbalanced-learn 라이브러리를 설치\n",
        "! pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-N6wIghFDXl",
        "outputId": "02748870-abbb-4c4d-956c-0483e5f7b20c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE(Synthetic Minority Over-sampling Technique)를 사용하여 불균형한 데이터셋을 처리\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
        "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)\n",
        "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)\n",
        "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_train_over).value_counts())"
      ],
      "metadata": {
        "id": "lDPYCMA5GIIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  SMOTE를 적용한 학습 데이터셋을 사용하여 로지스틱 회귀 모델을 학습하고 예측 성능을 평가\n",
        "lr_clf = LogisticRegression()\n",
        "# ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의\n",
        "get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "2dxwZA7CGvZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision-Recall 커브를 시각화하는 함수\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def precision_recall_curve_plot(y_test , pred_proba_c1):\n",
        "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출.\n",
        "    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n",
        "\n",
        "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
        "    plt.figure(figsize=(8,6))\n",
        "    threshold_boundary = thresholds.shape[0]\n",
        "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
        "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
        "\n",
        "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
        "    start, end = plt.xlim()\n",
        "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
        "\n",
        "    # x축, y축 label과 legend, 그리고 grid 설정\n",
        "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
        "    plt.legend(); plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kHMuMeiQGwZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision-Recall 커브를 시각화하는 함수\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def precision_recall_curve_plot(y_test , pred_proba_c1):\n",
        "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출.\n",
        "    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n",
        "\n",
        "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
        "    plt.figure(figsize=(8,6))\n",
        "    threshold_boundary = thresholds.shape[0]\n",
        "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
        "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
        "\n",
        "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
        "    start, end = plt.xlim()\n",
        "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
        "\n",
        "    # x축, y축 label과 legend, 그리고 grid 설정\n",
        "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
        "    plt.legend(); plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GYC1ineOGyWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE(Synthetic Minority Over-sampling Technique)를 적용한 후에 정밀도가 낮고 재현율이 높은 문제가 발생할 수 있습니다. SMOTE는 소수 클래스 데이터를 인위적으로 생성하여 클래스 불균형을 해결하는 기법입니다. 그러나 이로 인해 모델이 양성(Positive) 클래스를 과도하게 예측하게 되는 경우가 생길 수 있습니다. 이는 많은 거짓 양성(False Positive)을 초래할 수 있으며, 결과적으로 정밀도가 낮아질 수 있습니다."
      ],
      "metadata": {
        "id": "mKDasBViG0eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision_recall_curve_plot( y_test, lr_clf.predict_proba(X_test)[:, 1] )"
      ],
      "metadata": {
        "id": "eBotRzd6IJy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM이 SMOTE로 증강된 데이터를 사용하여 모델링할 때 정밀도가 정상이 되는 이유는 다음과 같습니다:\n",
        "- LightGBM의 강력한 성능: 데이터 처리와 피처 중요도를 잘 반영하는 학습 방식.\n",
        "- SMOTE와 LightGBM의 시너지 효과: 클래스 불균형 문제를 해결하여 균형 잡힌 모델 학습.\n",
        "- 모델 튜닝 및 하이퍼파라미터 최적화: 적절한 하이퍼파라미터 설정을 통해 모델 성능 향상"
      ],
      "metadata": {
        "id": "kMeg3LbZHNxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test,\n",
        "                  tgt_train=y_train_over, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "LhMNFgb3G06D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}