{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task1_0723. 'Breast Cancer Wisconsin (Diagnostic) Data Set'을 사용하여 이진 분류 문제를 해결하고, 평가 지표(정확도, 정밀도, 재현율, F1 스코어, ROC AUC)를 계산하세요."
      ],
      "metadata": {
        "id": "8arVdMK5Qn4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 데이터 로드\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "\n",
        "def tranin_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "  # 데이터 표준화\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "  # 로지스틱 회귀 모델 학습\n",
        "  lr_clf = LogisticRegression(max_iter=500, solver='lbfgs', random_state=42)\n",
        "  lr_clf.fit(X_train_scaled, y_train)\n",
        "  pred = lr_clf.predict(X_test_scaled)\n",
        "  y_proba = lr_clf.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "  # 성능평가\n",
        "  accuracy = accuracy_score(y_test, pred)\n",
        "  precision = precision_score(y_test, pred)\n",
        "  recall = recall_score(y_test, pred)\n",
        "  f1 = f1_score(y_test, pred)\n",
        "  roc_auc = roc_auc_score(y_test, y_proba) # 예측의 양성 확률을 넣어서 구한다.\n",
        "\n",
        "  print(f\"Accuracy: {accuracy:.2f}\")\n",
        "  print(f\"Precision: {precision:.2f}\")\n",
        "  print(f\"Recall: {recall:.2f}\")\n",
        "  print(f\"F1-score: {f1:.2f}\")\n",
        "  print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "\n",
        "# 여러 모델 훈련 및 평가\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=500, solver='lbfgs', random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Support Vector Machine': SVC(probability=True, random_state=42) # probability=True는 확률 값을 반환하게 하여 predict_proba 메서드를 사용할 수 있게 함\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "  print(f'[{model_name}]')\n",
        "  train_and_evaluate(model, X_train, X_test, y_train, y_test)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "kaq1pjREpmds",
        "outputId": "2b9f7b45-0e6f-4953-b6e1-04687201efd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Logistic Regression]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f59fdc5e7daf>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[{model_name}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mtranin_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task2_0723.\n",
        "가상의 데이터셋을 생성하고, 이를 사용하여 다중 클래스 분류 모델을 훈련시킨 후 평가 지표를 계산하세요. 평가 지표는 정확도, 정밀도, 재현율, F1 스코어, ROC AUC입니다.\n",
        "\n",
        "- n_samples=1500: 데이터셋에 포함될 샘플의 수를 1500개로 지정합니다.\n",
        "- n_features=20: 각 샘플이 가질 특성(feature)의 수를 20개로 지정합니다.\n",
        "- n_classes=5: 타겟 라벨의 클래스 수를 5개로 지정합니다.\n",
        "- n_informative=15: 20개의 특성 중 15개는 타겟 라벨과 관련된 유용한 정보를 포함하도록 지정합니다. 나머지 5개의 특성은 유용하지 않거나 무작위로 생성된 특성입니다.\n",
        "- random_state=42: 재현성을 위해 난수 생성 seed를 설정합니다."
      ],
      "metadata": {
        "id": "JMdIFXAfQqFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "average 매개변수에는 여러 가지 옵션이 있으며, 각 옵션은 다중 클래스 데이터에 대한 정밀도를 계산하는 다른 방법을 제공\n",
        "\n",
        "[ average 매개변수 옵션 ]\n",
        "- average='macro':\n",
        "각 클래스의 정밀도를 개별적으로 계산한 후, 단순 평균을 구합니다.\n",
        "모든 클래스가 동일하게 가중치를 부여받습니다.\n",
        "클래스 간 불균형이 있을 때 유용합니다.\n",
        "- average='micro':\n",
        "전체 TP, FP, FN을 합쳐서 정밀도를 계산합니다.\n",
        "모든 샘플을 개별적으로 동일하게 취급한다.\n",
        "- average='weighted':\n",
        "각 클래스의 정밀도를 개별적으로 계산한 후, 클래스별 샘플 수로 가중 평균을 구합니다.\n",
        "클래스의 샘플 수에 따라 가중치를 부여합니다.\n",
        "- average='samples' (다중 레이블 분류에 사용됨):\n",
        "각 샘플에 대해 개별적으로 메트릭을 계산한 후 평균을 구합니다.\n",
        "- average=None:\n",
        "각 클래스별로 정밀도를 반환합니다. 다중 클래스 분류에서 각 클래스에 대한 정밀도를 별도로 얻을 수 있습니다."
      ],
      "metadata": {
        "id": "enwPrZhfNUf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 데이터 생성 (5개의 클래스)\n",
        "X, y = make_classification(n_samples=1500, n_features=20, n_classes=5, n_informative=15, random_state=42)\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 다중클래스 분류를 위한 랜덤 포레스트 모델 생성 및 학습\n",
        "forest_model = RandomForestClassifier()\n",
        "forest_model.fit(X_train, y_train)\n",
        "\n",
        "pred = forest_model.predict(X_test)\n",
        "\n",
        "# 다중 클래스 라벨 이진화\n",
        "y_test_binarized = label_binarize(y_test, classes=[0,1,2,3,4])\n",
        "y_proba = forest_model.predict_proba(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "precision = precision_score(y_test, pred, average='macro')\n",
        "recall = recall_score(y_test, pred, average='macro')\n",
        "f1 = f1_score(y_test, pred, average='macro')\n",
        "roc_auc = roc_auc_score(y_test_binarized, y_proba, average='macro', multi_class='ovr')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "WwXrAGuPQrlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "           'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "# na_values = ? 는 ?로 되어있는 값들을 None값으로 처리한다는 의미\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values='?', skipinitialspace=True)"
      ],
      "metadata": {
        "id": "jUY5dtWfP5qt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['marital-status'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onrstAbkP7fx",
        "outputId": "229ea9c8-ba93-496a-aae3-bb4d4d7979d8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Never-married', 'Married-civ-spouse', 'Divorced',\n",
              "       'Married-spouse-absent', 'Separated', 'Married-AF-spouse',\n",
              "       'Widowed'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['relationship'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_NhFwUBP_mQ",
        "outputId": "2fdd1e6f-2ae4-47e9-c71d-22151a63fd43"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Not-in-family', 'Husband', 'Wife', 'Own-child', 'Unmarried',\n",
              "       'Other-relative'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제 3"
      ],
      "metadata": {
        "id": "CaYf-RTn2HcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. 데이터 로드\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "           'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "# na_values = ? 는 ?로 되어있는 값들을 None값으로 처리한다는 의미\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values='?', skipinitialspace=True)\n",
        "\n",
        "# 결측값 처리\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# fnlwgt와 capital-gain 이상치 제거\n",
        "Q1 = data['fnlwgt'].quantile(0.25)\n",
        "Q3 = data['fnlwgt'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_fnlwgt_outliers = data[(data['fnlwgt'] < lower_bound) | (data['fnlwgt'] > upper_bound)]\n",
        "data = data.drop(capital_fnlwgt_outliers.index)\n",
        "\n",
        "Q1 = data['capital-gain'].quantile(0.25)\n",
        "Q3 = data['capital-gain'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "capital_gain_outliers = data[(data['capital-gain'] < lower_bound) | (data['capital-gain'] > upper_bound)]\n",
        "capital_loss_outliers = data[(data['capital-loss'] < lower_bound) | (data['capital-loss'] > upper_bound)]\n",
        "data = data.drop(capital_gain_outliers.index)\n",
        "data = data.drop(capital_loss_outliers.index)\n",
        "\n",
        "# 파생변수1 : age_group\n",
        "data['age_group'] = pd.cut(data['age'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '61+'])\n",
        "\n",
        "# 파생변수2 : hours_group\n",
        "data['hours_group'] = pd.cut(data['hours-per-week'], bins=[0, 20, 40, 60, 100], labels=['0-20', '21-40', '41-60', '61+'])\n",
        "\n",
        "# 범주형 변수 인코딩\n",
        "categorical_features = ['age_group', 'hours_group', 'race', 'sex', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'native-country']\n",
        "data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "# 파생변수3 : capital\n",
        "data['capital'] = data['capital-gain'] - data['capital-loss']\n",
        "\n",
        "# 불필요 컬럼 제거\n",
        "data.drop(columns=['education-num', 'capital-gain', 'capital-loss', 'age', 'hours-per-week'], inplace=True)\n",
        "\n",
        "data['income'] = data['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)\n",
        "X = data.drop('income', axis=1)\n",
        "y = data['income']\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 결정 트리 모델 학습\n",
        "tree = DecisionTreeClassifier(max_depth=15, min_samples_split=15, min_samples_leaf=15, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# 특성 중요도 추출\n",
        "feature_importances = tree.feature_importances_\n",
        "\n",
        "# 중요도가 높은 순으로 특성 정렬\n",
        "important_features = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# 상위 N개의 중요 특성 선택 (예: 상위 10개 특성)\n",
        "N = 12\n",
        "selected_features = important_features[:N]\n",
        "\n",
        "# 선택된 특성으로 데이터셋 구성\n",
        "X_train_selected = X_train.iloc[:, selected_features]\n",
        "X_test_selected = X_test.iloc[:, selected_features]\n",
        "\n",
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_selected)\n",
        "X_test = scaler.transform(X_test_selected)\n",
        "\n",
        "# 9. Logistic Regression 모델 생성 및 학습\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 10. 예측 및 평가\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "-28t2bx6SB2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "56b43dc1-daf9-4d0a-8064-996f9343bbd9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# 1. 데이터 로드\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "           'hours-per-week', 'native-country', 'income']\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values=' ?', skipinitialspace=True)\n",
        "\n",
        "# 2. 결측치 처리\n",
        "# 수치형 변수의 결측치를 중앙값으로 대체\n",
        "numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "imputer_numeric = SimpleImputer(strategy='median')\n",
        "data[numeric_features] = imputer_numeric.fit_transform(data[numeric_features])\n",
        "\n",
        "# 범주형 변수의 결측치를 최빈값으로 대체\n",
        "categorical_features = data.select_dtypes(include=[object]).columns.tolist()\n",
        "categorical_features.remove('income')\n",
        "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "data[categorical_features] = imputer_categorical.fit_transform(data[categorical_features])\n",
        "\n",
        "# 3. 이상치 제거 (여기서는 'capital-gain'과 'capital-loss'에서 극단적인 값들을 이상치로 가정)\n",
        "def replace_outliers_with_median(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    median = df[column].median()\n",
        "    df[column] = np.where((df[column] < lower_bound) | (df[column] > upper_bound), median, df[column])\n",
        "    return df\n",
        "\n",
        "for col in numeric_features:\n",
        "    data = replace_outliers_with_median(data, col)\n",
        "\n",
        "# 4. 파생변수 작성\n",
        "data['capital_diff'] = data['capital-gain'] - data['capital-loss']\n",
        "\n",
        "# 5. 범주형 변수 인코딩\n",
        "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "encoded_categorical_data = encoder.fit_transform(data[categorical_features])\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_features))\n",
        "\n",
        "# 원래 데이터프레임에서 범주형 열을 제거하고 인코딩된 데이터프레임을 병합\n",
        "data = data.drop(columns=categorical_features)\n",
        "data = pd.concat([data, encoded_categorical_df], axis=1)\n",
        "\n",
        "# 6. 변수 선택 및 데이터 분리\n",
        "# 'income' 변수를 0과 1로 변환\n",
        "data['income'] = data['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)\n",
        "X = data.drop('income', axis=1)\n",
        "y = data['income']\n",
        "\n",
        "# 7. 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 8. 학습용과 테스트용 데이터셋으로 나누기\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 9. Logistic Regression 모델 생성 및 학습\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 10. 예측 및 평가\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "print('Classification Report:')\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "DRCyz1XHd5sL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}